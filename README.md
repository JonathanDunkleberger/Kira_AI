# ğŸ§  Kira AI

![Demo of Kira AI in action](https://github.com/JonathanDunkleberger/Kira_AI/blob/main/VTuber%20Demo%20-%20Kirav3.gif?raw=true)

**A multimodal, memory-persistent AI agent that listens, sees, speaks, and acts â€” running entirely on local hardware.**

Kira is not a chatbot. She is a real-time cognitive agent with **long-term semantic memory**, **computer vision**, **voice interaction**, and **proactive autonomous behavior**. She runs a local LLM on consumer GPU hardware, remembers facts about her user across sessions, watches the screen to understand context, and integrates with live platforms like Twitch â€” all without sending private data to the cloud.

This project demonstrates end-to-end systems design: real-time audio pipelines, vector-database memory architectures, multimodal sensor fusion, and agentic decision loops â€” built from scratch in Python.

---

## ğŸ—ï¸ Architecture

```mermaid
flowchart TD
    subgraph Inputs
        MIC[ğŸ¤ Microphone]
        SCREEN[ğŸ–¥ï¸ Screen Capture]
        TWITCH[ğŸ’¬ Twitch Chat]
    end

    subgraph Brain ["ğŸ§  Cognitive Core"]
        STT[Faster-Whisper STT]
        QUEUE[Input Queue]
        BRAIN[Brain Worker]
        LLM[Llama 3 LLM]
        EMOTION[Emotion Analyzer]
    end

    subgraph Memory ["ğŸ’¾ Memory System"]
        EXTRACT[Fact Extractor]
        CHROMA[(ChromaDB)]
        SUMMARY[Summarizer]
    end

    subgraph Perception ["ğŸ‘ï¸ Perception"]
        VISION[Vision Agent]
        GMC[Observer Mode]
    end

    subgraph Outputs
        TTS[ğŸ”Š Azure / ElevenLabs TTS]
        TOOLS[ğŸ› ï¸ Polls Â· Music Â· Search]
    end

    MIC --> STT --> QUEUE
    TWITCH --> QUEUE
    SCREEN --> VISION

    QUEUE --> BRAIN
    VISION -->|context injection| BRAIN
    CHROMA -->|relevant memories| BRAIN
    BRAIN --> LLM --> TTS
    LLM --> TOOLS
    LLM --> EMOTION -->|updates state| BRAIN

    BRAIN -->|voice turns| EXTRACT --> CHROMA
    BRAIN -->|segments| SUMMARY --> CHROMA

    GMC -->|toggles| VISION
```

---

## âœ¨ What Makes This Interesting

### âš¡ Local-First Inference
Runs **Llama 3.1 (8B, Q4_K_M)** entirely on-device via `llama-cpp-python` with Flash Attention enabled. No API calls for core reasoning â€” conversations stay private and latency stays low.

### ğŸ§  Persistent Semantic Memory
A **ChromaDB** vector database stores extracted facts (not raw transcripts) across sessions. The memory extractor uses the LLM itself to distill durable knowledge from conversation ("Jonny's favorite anime is Steins;Gate") and injects only relevant memories into each prompt via semantic retrieval.

### ğŸ‘ï¸ Multimodal Perception
A **Vision Agent** captures the screen, describes it via a Vision LLM (GPT-4o-mini), and injects that context into the cognitive stream. The agent understands whether you're coding, gaming, or watching a video â€” and adapts accordingly.

### ğŸ—£ï¸ Full Voice Pipeline
Real-time **Voice Activity Detection** (WebRTC VAD) â†’ **Faster-Whisper** transcription â†’ LLM reasoning â†’ **Azure Neural TTS** speech output. Includes self-hearing prevention, interruption handling, and TTS rate limiting.

### ğŸ¤– Proactive Agency
Kira doesn't just respond â€” she **initiates**. A background observer loop monitors silence duration and escalates through behavioral stages (casual check-in â†’ provocation â†’ chaos). Vision heartbeats let her comment on what she sees without being asked.

### ğŸ® Platform Integration
- **Twitch**: Reads chat, responds contextually, creates polls, handles song requests.
- **Music**: Searches YouTube and streams audio via `mpv` on natural language request.
- **Web Search**: Autonomous Google queries when she doesn't know something.

---

## ğŸ“ Project Structure

| File | Role | Key Dependencies |
|------|------|-----------------|
| `bot.py` | **Orchestrator** â€” Event loop, VAD, input queue, brain worker | `pyaudio`, `webrtcvad` |
| `ai_core.py` | **Cortex** â€” LLM inference, STT, TTS, prompt assembly | `llama-cpp-python`, `faster-whisper`, `azure-cognitiveservices-speech` |
| `memory.py` | **Hippocampus** â€” ChromaDB interface, semantic retrieval | `chromadb`, `sentence-transformers` |
| `memory_extractor.py` | **Fact Extraction** â€” Distills durable facts from conversation | LLM tool inference |
| `summarizer.py` | **Consolidation** â€” Periodically summarizes conversation segments into memory | LLM tool inference |
| `vision_agent.py` | **Eyes** â€” Screen capture, VLM description, context buffer | `Pillow`, `openai` |
| `dashboard.py` | **GUI** â€” Real-time controls, vision preview, state monitoring | `customtkinter` |
| `game_mode_controller.py` | **Mode Toggle** â€” Toggles vision observer mode on/off | â€” |
| `twitch_bot.py` | **Twitch Client** â€” Chat listener, song request handler | `twitchio` |
| `twitch_tools.py` | **Twitch API** â€” Poll creation, broadcaster utilities | `requests` |
| `music_tools.py` | **DJ** â€” YouTube search and `mpv` audio streaming | `yt-dlp` |
| `web_search.py` | **Search** â€” Google Custom Search API wrapper | `google-api-python-client` |
| `persona.py` | **Emotional State** â€” Enum of moods that influence response style | â€” |
| `personality.txt` | **Identity** â€” Natural language personality prompt (source of truth) | â€” |
| `prompt_rules.py` | **Formatting Rules** â€” Output constraints and tool tag definitions | â€” |
| `config.py` | **Configuration** â€” All settings loaded from `.env` | `python-dotenv` |

---

## ğŸš€ Setup

### Prerequisites
- Python 3.10+
- NVIDIA GPU (RTX 3060+ recommended) with CUDA drivers
- [Visual Studio Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/) (C++ tools, required for `llama-cpp-python`)
- `mpv` (for music playback)

### Quick Start

```bash
git clone https://github.com/JonathanDunkleberger/Kira_AI.git
cd Kira_AI
pip install -r requirements.txt
```

Download a GGUF model (e.g., [`Meta-Llama-3.1-8B-Instruct-Q4_K_M`](https://huggingface.co/)) and place it in `models/`.

```bash
cp .env.example .env   # Fill in your API keys
python dashboard.py     # Launch the GUI + bot
```

---

## ğŸ› ï¸ Customization

| What | Where | How |
|------|-------|-----|
| Personality & backstory | `personality.txt` | Edit the natural language prompt directly |
| Emotional states | `persona.py` | Add/modify the `EmotionalState` enum |
| Output formatting rules | `prompt_rules.py` | Adjust constraints (length, style, tool tags) |
| All runtime settings | `.env` | API keys, model paths, TTS engine, feature flags |

---

## ğŸ”® Roadmap

- **GraphRAG** â€” Graph-based memory for richer relationship tracking between facts
- **Local Vision** â€” Replace API vision with a quantized LLaVA model for full offline capability
- **Live2D Integration** â€” WebSocket bridge to drive avatar expressions from emotional state
- **Multi-agent Reasoning** â€” Separate planning and execution into cooperative agent threads
